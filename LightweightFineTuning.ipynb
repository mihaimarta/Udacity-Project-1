{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7549e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad741096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from transformers.utils import logging\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM, AutoPeftModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "\n",
    "# Split the train set into train/validation\n",
    "train_valid = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Inspect the dataset\n",
    "print(\"Train sample:\", train_valid[\"train\"][0])\n",
    "print(\"Validation sample:\", train_valid[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_train = train_valid[\"train\"].map(tokenize, batched=True)\n",
    "tokenized_test = train_valid[\"test\"].map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 4\n",
    "id2label = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger'}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Check the result\n",
    "print(train_valid['train'][0])\n",
    "print(train_valid['test'][0])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the model parameters\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Print parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_params:,} total parameters, including {total_trainable_params:,} trainable parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b60aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "tokenized_train = train_valid[\"train\"].map(tokenize, batched=True)\n",
    "tokenized_test = train_valid[\"test\"].map(tokenize, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{model_name}/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=300,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\", \n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83604aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use label_list, label2id, id2label from earlier preprocessing\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Use label_list, label2id, id2label from your preprocessing cell\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Define the LoRA configuration\n",
    "config = LoraConfig(\n",
    "    task_type='SEQ_CLS',\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"],\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "fine_tuned_model = get_peft_model(model, config)\n",
    "\n",
    "# Print trainable parameters\n",
    "fine_tuned_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{model_name}-lora/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=300,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "lora_trainer = Trainer(\n",
    "    model=fine_tuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the loRA model\n",
    "fine_tuned_model.save_pretrained(f\"./peft/{model_name}-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edecf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved PEFT (LoRA) model weights and evaluate performance\n",
    "loaded_lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    f\"./peft/{model_name}-lora\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    torch_dtype=torch.float16  # Add this line for faster loading if your hardware supports it\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "loaded_lora_model = loaded_lora_model.to(device)\n",
    "\n",
    "# Evaluate on the test set\n",
    "lora_eval_trainer = Trainer(\n",
    "    model=loaded_lora_model,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "lora_eval_results = lora_eval_trainer.evaluate()\n",
    "print(\"PEFT Model Evaluation Results:\", lora_eval_results)\n",
    "\n",
    "# For comparison, print the base model evaluation results if available\n",
    "try:\n",
    "    print(\"Base Model Evaluation Results:\", trainer.evaluate())\n",
    "except Exception as e:\n",
    "    print(\"Base model evaluation not available or failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print only the accuracy comparison between PEFT and base models\n",
    "peft_accuracy = lora_eval_results.get('eval_accuracy', None)\n",
    "try:\n",
    "    base_results = trainer.evaluate()\n",
    "    base_accuracy = base_results.get('eval_accuracy', None)\n",
    "except Exception as e:\n",
    "    base_accuracy = None\n",
    "\n",
    "print(\"Accuracy Comparison:\")\n",
    "print(f\"PEFT Model Accuracy: {peft_accuracy}\")\n",
    "if base_accuracy is not None:\n",
    "    print(f\"Base Model Accuracy: {base_accuracy}\")\n",
    "else:\n",
    "    print(\"Base model accuracy not available or failed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
